{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import rawpy\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import LabeledDataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import ignite.distributed as idist\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.metrics import FID, InceptionScore, RunningAverage\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 09:58:05,365 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<utils.datasets.Labe': \n",
      "\t{'batch_size': 4, 'num_workers': 8, 'shuffle': True, 'drop_last': True, 'prefetch_factor': 1, 'pin_memory': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2848, 4256])\n",
      "torch.Size([3, 2848, 4256])\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"dataset\"\n",
    "sony_csv_files = [\"dataset/Sony_train_list.txt\"]\n",
    "fuji_csv_files =  [\"dataset/Fuji_train_list.txt\"]\n",
    "\n",
    "batch_size = 4\n",
    "input_size = 512\n",
    "\n",
    "pre_crop_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "sony_dataset = LabeledDataset(root_dir, *sony_csv_files, transform=pre_crop_transform)\n",
    "sony_dataloader = idist.auto_dataloader(sony_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True, prefetch_factor=1)\n",
    "print(sony_dataset[0][0].shape)\n",
    "print(sony_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sony_dataset.prime_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unet.unet_model import UNet\n",
    "from torch import optim \n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.lrelu1_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel, stride=stride,  padding=padding)\n",
    "        self.lrelu1_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.lrelu1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.lrelu1_2(x)\n",
    "        return x\n",
    "    \n",
    "class UpConcatBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UpConcatBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channel, in_channel // 2, kernel_size=2, stride=2)\n",
    "        self.conv_block = ConvBlock(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.deconv(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.down1 = ConvBlock(in_feat, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = ConvBlock(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = ConvBlock(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.down5 = ConvBlock(256, 512)\n",
    "\n",
    "        self.up5 = UpConcatBlock(512, 256)\n",
    "        self.up4 = UpConcatBlock(256, 128)\n",
    "        self.up3 = UpConcatBlock(128, 64)\n",
    "        self.up2 = UpConcatBlock(64, 32)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, out_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(self.pool1(down1))\n",
    "        down3 = self.down3(self.pool2(down2))\n",
    "        down4 = self.down4(self.pool3(down3))\n",
    "        down5 = self.down5(self.pool4(down4))\n",
    "\n",
    "        up = self.up5(down5, down4)\n",
    "        up = self.up4(up, down3)\n",
    "        up = self.up3(up, down2)\n",
    "        up = self.up2(up, down1)\n",
    "\n",
    "        out = self.conv10(up)\n",
    "        out = torch.nn.functional.pixel_shuffle(out, 2)\n",
    "        return out\n",
    "    \n",
    "class UNet_D(nn.Module):\n",
    "    def __init__(self, in_feat):\n",
    "        super(UNet_D, self).__init__()\n",
    "        \n",
    "        self.down1 = ConvBlock(in_feat, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = ConvBlock(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = ConvBlock(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.down5 = ConvBlock(256, 512)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "        self.down6 = ConvBlock(512, 1024)\n",
    "        self.pool6 = nn.MaxPool2d(2)\n",
    "        self.down7 = ConvBlock(1024, 2048)\n",
    "        self.pool7 = nn.MaxPool2d(2)\n",
    "        self.down8 = ConvBlock(2048, 4096)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096*8*8, 1)\n",
    "\n",
    "        self.up8 = UpConcatBlock(4096, 2048)\n",
    "        self.up7 = UpConcatBlock(2048, 1024)\n",
    "        self.up6 = UpConcatBlock(1024, 512)\n",
    "        self.up5 = UpConcatBlock(512, 256)\n",
    "        self.up4 = UpConcatBlock(256, 128)\n",
    "        self.up3 = UpConcatBlock(128, 64)\n",
    "        self.up2 = UpConcatBlock(64, 32)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(self.pool1(down1))\n",
    "        down3 = self.down3(self.pool2(down2))\n",
    "        down4 = self.down4(self.pool3(down3))\n",
    "        down5 = self.down5(self.pool4(down4))\n",
    "        down6 = self.down6(self.pool5(down5))\n",
    "        down7 = self.down7(self.pool6(down6))\n",
    "        down8 = self.down8(self.pool7(down7))\n",
    "\n",
    "        down8_ = torch.flatten(down8, 1)\n",
    "        real_fake = self.fc1(down8_)\n",
    "\n",
    "        up = self.up8(down8, down7)\n",
    "        up = self.up7(up, down6)\n",
    "        up = self.up6(up, down5)\n",
    "        up = self.up5(up, down4)\n",
    "        up = self.up4(up, down3)\n",
    "        up = self.up3(up, down2)\n",
    "        up = self.up2(up, down1)\n",
    "\n",
    "        out = self.conv10(up)\n",
    "        return real_fake, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 09:58:07,769 ignite.distributed.auto.auto_model INFO: Apply torch DataParallel on model\n",
      "2023-06-10 09:58:10,226 ignite.distributed.auto.auto_model INFO: Apply torch DataParallel on model\n"
     ]
    }
   ],
   "source": [
    "netG = idist.auto_model(UNet(4, 12))\n",
    "netD = idist.auto_model(UNet_D(3))\n",
    "optimizerG = idist.auto_optim(optim.Adam(netG.parameters(), lr=1e-4))\n",
    "optimizerD = idist.auto_optim(optim.Adam(netD.parameters(), lr=1e-4))\n",
    "loss = nn.L1Loss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# lr_scheduler = LRScheduler(optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DataParallel                             [4, 3, 1024, 1024]        --\n",
       "├─UNet: 1-1                              [2, 3, 1024, 1024]        7,760,748\n",
       "├─UNet: 1-4                              --                        (recursive)\n",
       "│    └─ConvBlock: 2-1                    [2, 32, 512, 512]         10,432\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 512, 512]         1,184\n",
       "├─UNet: 1-3                              [2, 3, 1024, 1024]        --\n",
       "├─UNet: 1-4                              --                        (recursive)\n",
       "│    └─ConvBlock: 2-3                    [2, 32, 512, 512]         --\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-2                  [2, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-3               [2, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-4                  [2, 32, 512, 512]         9,248\n",
       "│    │    └─LeakyReLU: 3-5               [2, 32, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-5                    [2, 32, 256, 256]         --\n",
       "│    └─ConvBlock: 2-6                    [2, 64, 256, 256]         55,424\n",
       "│    └─ConvBlock: 2-29                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-6                  [2, 64, 256, 256]         18,496\n",
       "│    │    └─LeakyReLU: 3-7               [2, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-8                  [2, 64, 256, 256]         36,928\n",
       "│    │    └─LeakyReLU: 3-9               [2, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-8                    [2, 64, 128, 128]         --\n",
       "│    └─ConvBlock: 2-9                    [2, 128, 128, 128]        221,440\n",
       "│    └─ConvBlock: 2-32                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-10                 [2, 128, 128, 128]        73,856\n",
       "│    │    └─LeakyReLU: 3-11              [2, 128, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-12                 [2, 128, 128, 128]        147,584\n",
       "│    │    └─LeakyReLU: 3-13              [2, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-11                   [2, 128, 64, 64]          --\n",
       "│    └─ConvBlock: 2-12                   [2, 256, 64, 64]          885,248\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-14                 [2, 256, 64, 64]          295,168\n",
       "│    │    └─LeakyReLU: 3-15              [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-16                 [2, 256, 64, 64]          590,080\n",
       "│    │    └─LeakyReLU: 3-17              [2, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-14                   [2, 256, 32, 32]          --\n",
       "│    └─ConvBlock: 2-15                   [2, 512, 32, 32]          3,539,968\n",
       "│    └─ConvBlock: 2-38                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-18                 [2, 512, 32, 32]          1,180,160\n",
       "│    │    └─LeakyReLU: 3-19              [2, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-20                 [2, 512, 32, 32]          2,359,808\n",
       "│    │    └─LeakyReLU: 3-21              [2, 512, 32, 32]          --\n",
       "│    └─UpConcatBlock: 2-17               [2, 256, 64, 64]          2,294,528\n",
       "│    └─UpConcatBlock: 2-40               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-22        [2, 256, 64, 64]          524,544\n",
       "│    │    └─ConvBlock: 3-23              [2, 256, 64, 64]          1,769,984\n",
       "│    │    └─ConvBlock: 3-55              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-19               [2, 128, 128, 128]        573,824\n",
       "│    └─UpConcatBlock: 2-42               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-25        [2, 128, 128, 128]        131,200\n",
       "│    │    └─ConvBlock: 3-26              [2, 128, 128, 128]        442,624\n",
       "│    │    └─ConvBlock: 3-58              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-21               [2, 64, 256, 256]         143,552\n",
       "│    └─UpConcatBlock: 2-44               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-28        [2, 64, 256, 256]         32,832\n",
       "│    │    └─ConvBlock: 3-29              [2, 64, 256, 256]         110,720\n",
       "│    │    └─ConvBlock: 3-61              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-23               [2, 32, 512, 512]         35,936\n",
       "│    └─UpConcatBlock: 2-46               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-31        [2, 32, 512, 512]         8,224\n",
       "│    │    └─ConvBlock: 3-32              [2, 32, 512, 512]         27,712\n",
       "│    │    └─ConvBlock: 3-64              --                        (recursive)\n",
       "│    └─Conv2d: 2-25                      [2, 12, 512, 512]         396\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-34              [2, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-35                 [2, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-36              [2, 32, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-27                   [2, 32, 256, 256]         --\n",
       "│    └─ConvBlock: 2-28                   [2, 64, 256, 256]         --\n",
       "│    └─ConvBlock: 2-29                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-37                 [2, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-38              [2, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-39                 [2, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-40              [2, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-30                   [2, 64, 128, 128]         --\n",
       "│    └─ConvBlock: 2-31                   [2, 128, 128, 128]        --\n",
       "│    └─ConvBlock: 2-32                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-41                 [2, 128, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-42              [2, 128, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-43                 [2, 128, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-44              [2, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-33                   [2, 128, 64, 64]          --\n",
       "│    └─ConvBlock: 2-34                   [2, 256, 64, 64]          --\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-45                 [2, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-46              [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-47                 [2, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-48              [2, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-36                   [2, 256, 32, 32]          --\n",
       "│    └─ConvBlock: 2-37                   [2, 512, 32, 32]          --\n",
       "│    └─ConvBlock: 2-38                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-49                 [2, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-50              [2, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-51                 [2, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-52              [2, 512, 32, 32]          --\n",
       "│    └─UpConcatBlock: 2-39               [2, 256, 64, 64]          --\n",
       "│    └─UpConcatBlock: 2-40               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-53        [2, 256, 64, 64]          --\n",
       "│    │    └─ConvBlock: 3-54              [2, 256, 64, 64]          --\n",
       "│    │    └─ConvBlock: 3-55              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-41               [2, 128, 128, 128]        --\n",
       "│    └─UpConcatBlock: 2-42               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-56        [2, 128, 128, 128]        --\n",
       "│    │    └─ConvBlock: 3-57              [2, 128, 128, 128]        --\n",
       "│    │    └─ConvBlock: 3-58              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-43               [2, 64, 256, 256]         --\n",
       "│    └─UpConcatBlock: 2-44               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-59        [2, 64, 256, 256]         --\n",
       "│    │    └─ConvBlock: 3-60              [2, 64, 256, 256]         --\n",
       "│    │    └─ConvBlock: 3-61              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-45               [2, 32, 512, 512]         --\n",
       "│    └─UpConcatBlock: 2-46               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-62        [2, 32, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-63              [2, 32, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-64              --                        (recursive)\n",
       "│    └─Conv2d: 2-47                      [2, 12, 512, 512]         --\n",
       "==========================================================================================\n",
       "Total params: 25,632,888\n",
       "Trainable params: 25,632,888\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 109.96\n",
       "==========================================================================================\n",
       "Input size (MB): 16.78\n",
       "Forward/backward pass size (MB): 1325.40\n",
       "Params size (MB): 31.04\n",
       "Estimated Total Size (MB): 1373.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randn(batch_size, 4, input_size, input_size)\n",
    "summary(netG, input_data = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DataParallel                             [4, 1]                    --\n",
       "├─UNet_D: 1-1                            [2, 1]                    497,994,466\n",
       "├─UNet_D: 1-4                            --                        (recursive)\n",
       "│    └─ConvBlock: 2-1                    [2, 32, 1024, 1024]       10,144\n",
       "│    └─ConvBlock: 2-8                    --                        (recursive)\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 1024, 1024]       896\n",
       "├─UNet_D: 1-3                            [2, 1]                    --\n",
       "├─UNet_D: 1-4                            --                        (recursive)\n",
       "│    └─ConvBlock: 2-3                    [2, 32, 1024, 1024]       --\n",
       "│    └─ConvBlock: 2-8                    --                        (recursive)\n",
       "│    │    └─Conv2d: 3-2                  [2, 32, 1024, 1024]       --\n",
       "│    │    └─LeakyReLU: 3-3               [2, 32, 1024, 1024]       --\n",
       "│    │    └─Conv2d: 3-4                  [2, 32, 1024, 1024]       9,248\n",
       "│    │    └─LeakyReLU: 3-5               [2, 32, 1024, 1024]       --\n",
       "│    │    └─Conv2d: 3-6                  [2, 32, 1024, 1024]       --\n",
       "│    │    └─LeakyReLU: 3-7               [2, 32, 1024, 1024]       --\n",
       "│    └─MaxPool2d: 2-5                    [2, 32, 512, 512]         --\n",
       "│    └─ConvBlock: 2-6                    [2, 64, 512, 512]         55,424\n",
       "│    └─ConvBlock: 2-17                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-8                  [2, 64, 512, 512]         18,496\n",
       "│    │    └─LeakyReLU: 3-9               [2, 64, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-10                 [2, 64, 512, 512]         36,928\n",
       "│    └─ConvBlock: 2-8                    --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-11              [2, 32, 1024, 1024]       --\n",
       "│    └─MaxPool2d: 2-9                    [2, 32, 512, 512]         --\n",
       "│    └─ConvBlock: 2-10                   [2, 64, 512, 512]         --\n",
       "│    └─ConvBlock: 2-17                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-12                 [2, 64, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-13              [2, 64, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-12                   [2, 64, 256, 256]         --\n",
       "│    └─ConvBlock: 2-13                   [2, 128, 256, 256]        221,440\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-14                 [2, 128, 256, 256]        73,856\n",
       "│    └─ConvBlock: 2-17                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-15              [2, 64, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-16                 [2, 64, 512, 512]         --\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-17              [2, 128, 256, 256]        --\n",
       "│    │    └─Conv2d: 3-18                 [2, 128, 256, 256]        147,584\n",
       "│    └─ConvBlock: 2-17                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-19              [2, 64, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-18                   [2, 64, 256, 256]         --\n",
       "│    └─ConvBlock: 2-19                   [2, 128, 256, 256]        --\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-20                 [2, 128, 256, 256]        --\n",
       "│    │    └─LeakyReLU: 3-21              [2, 128, 256, 256]        --\n",
       "│    └─MaxPool2d: 2-21                   [2, 128, 128, 128]        --\n",
       "│    └─ConvBlock: 2-22                   [2, 256, 128, 128]        885,248\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-22                 [2, 256, 128, 128]        295,168\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-23              [2, 128, 256, 256]        --\n",
       "│    │    └─Conv2d: 3-24                 [2, 128, 256, 256]        --\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-25              [2, 256, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-26                 [2, 256, 128, 128]        590,080\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-27              [2, 128, 256, 256]        --\n",
       "│    └─MaxPool2d: 2-27                   [2, 128, 128, 128]        --\n",
       "│    └─ConvBlock: 2-28                   [2, 256, 128, 128]        --\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-28                 [2, 256, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-29              [2, 256, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-30                   [2, 256, 64, 64]          --\n",
       "│    └─ConvBlock: 2-31                   [2, 512, 64, 64]          3,539,968\n",
       "│    └─ConvBlock: 2-44                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-30                 [2, 512, 64, 64]          1,180,160\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-31              [2, 256, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-32                 [2, 256, 128, 128]        --\n",
       "│    └─ConvBlock: 2-44                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-33              [2, 512, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-34                 [2, 512, 64, 64]          2,359,808\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-35              [2, 256, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-36                   [2, 256, 64, 64]          --\n",
       "│    └─ConvBlock: 2-37                   [2, 512, 64, 64]          --\n",
       "│    └─ConvBlock: 2-44                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-36                 [2, 512, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-37              [2, 512, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-39                   [2, 512, 32, 32]          --\n",
       "│    └─ConvBlock: 2-40                   [2, 1024, 32, 32]         14,157,824\n",
       "│    └─ConvBlock: 2-51                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-38                 [2, 1024, 32, 32]         4,719,616\n",
       "│    └─ConvBlock: 2-44                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-39              [2, 512, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-40                 [2, 512, 64, 64]          --\n",
       "│    └─ConvBlock: 2-51                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-41              [2, 1024, 32, 32]         --\n",
       "│    │    └─Conv2d: 3-42                 [2, 1024, 32, 32]         9,438,208\n",
       "│    └─ConvBlock: 2-44                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-43              [2, 512, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-45                   [2, 512, 32, 32]          --\n",
       "│    └─ConvBlock: 2-46                   [2, 1024, 32, 32]         --\n",
       "│    └─ConvBlock: 2-51                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-44                 [2, 1024, 32, 32]         --\n",
       "│    │    └─LeakyReLU: 3-45              [2, 1024, 32, 32]         --\n",
       "│    │    └─Conv2d: 3-46                 [2, 1024, 32, 32]         --\n",
       "│    │    └─LeakyReLU: 3-47              [2, 1024, 32, 32]         --\n",
       "│    └─MaxPool2d: 2-48                   [2, 1024, 16, 16]         --\n",
       "│    └─ConvBlock: 2-49                   [2, 2048, 16, 16]         56,627,200\n",
       "│    └─ConvBlock: 2-59                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-48                 [2, 2048, 16, 16]         18,876,416\n",
       "│    └─ConvBlock: 2-51                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-49              [2, 1024, 32, 32]         --\n",
       "│    └─MaxPool2d: 2-52                   [2, 1024, 16, 16]         --\n",
       "│    └─ConvBlock: 2-53                   [2, 2048, 16, 16]         --\n",
       "│    └─ConvBlock: 2-59                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-50                 [2, 2048, 16, 16]         --\n",
       "│    │    └─LeakyReLU: 3-51              [2, 2048, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-52                 [2, 2048, 16, 16]         37,750,784\n",
       "│    │    └─LeakyReLU: 3-53              [2, 2048, 16, 16]         --\n",
       "│    │    └─LeakyReLU: 3-54              [2, 2048, 16, 16]         --\n",
       "│    └─MaxPool2d: 2-55                   [2, 2048, 8, 8]           --\n",
       "│    └─ConvBlock: 2-59                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-55                 [2, 2048, 16, 16]         --\n",
       "│    └─ConvBlock: 2-57                   [2, 4096, 8, 8]           226,500,608\n",
       "│    └─ConvBlock: 2-74                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-56                 [2, 4096, 8, 8]           75,501,568\n",
       "│    │    └─LeakyReLU: 3-57              [2, 4096, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-58                 [2, 4096, 8, 8]           150,999,040\n",
       "│    └─ConvBlock: 2-59                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-59              [2, 2048, 16, 16]         --\n",
       "│    └─MaxPool2d: 2-60                   [2, 2048, 8, 8]           --\n",
       "│    └─ConvBlock: 2-61                   [2, 4096, 8, 8]           --\n",
       "│    └─ConvBlock: 2-74                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-60                 [2, 4096, 8, 8]           --\n",
       "│    │    └─LeakyReLU: 3-61              [2, 4096, 8, 8]           --\n",
       "│    └─Linear: 2-63                      [2, 1]                    262,145\n",
       "│    └─UpConcatBlock: 2-64               [2, 2048, 16, 16]         146,806,784\n",
       "│    └─UpConcatBlock: 2-83               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-62        [2, 2048, 16, 16]         33,556,480\n",
       "│    │    └─ConvBlock: 3-63              [2, 2048, 16, 16]         113,250,304\n",
       "│    │    └─ConvBlock: 3-84              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-66               [2, 1024, 32, 32]         36,703,232\n",
       "│    └─UpConcatBlock: 2-91               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-65        [2, 1024, 32, 32]         8,389,632\n",
       "│    │    └─ConvBlock: 3-66              [2, 1024, 32, 32]         28,313,600\n",
       "│    │    └─ConvBlock: 3-92              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-68               [2, 512, 64, 64]          9,176,576\n",
       "│    └─UpConcatBlock: 2-97               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-68        [2, 512, 64, 64]          2,097,664\n",
       "│    └─ConvBlock: 2-74                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-69              [2, 4096, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-70                 [2, 4096, 8, 8]           --\n",
       "│    └─UpConcatBlock: 2-97               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-71              [2, 512, 64, 64]          7,078,912\n",
       "│    │    └─ConvBlock: 3-97              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-72               [2, 256, 128, 128]        2,294,528\n",
       "│    └─UpConcatBlock: 2-104              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-73        [2, 256, 128, 128]        524,544\n",
       "│    │    └─ConvBlock: 3-74              [2, 256, 128, 128]        1,769,984\n",
       "│    │    └─ConvBlock: 3-104             --                        (recursive)\n",
       "│    └─ConvBlock: 2-74                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-76              [2, 4096, 8, 8]           --\n",
       "│    └─Linear: 2-75                      [2, 1]                    --\n",
       "│    └─UpConcatBlock: 2-76               [2, 2048, 16, 16]         --\n",
       "│    └─UpConcatBlock: 2-83               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-77        [2, 2048, 16, 16]         --\n",
       "│    └─UpConcatBlock: 2-104              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-104             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-79               [2, 128, 256, 256]        573,824\n",
       "│    └─UpConcatBlock: 2-106              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-79        [2, 128, 256, 256]        131,200\n",
       "│    └─UpConcatBlock: 2-83               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-80              [2, 2048, 16, 16]         --\n",
       "│    │    └─ConvBlock: 3-84              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-106              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-82              [2, 128, 256, 256]        442,624\n",
       "│    │    └─ConvBlock: 3-107             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-83               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-84              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-84               [2, 1024, 32, 32]         --\n",
       "│    └─UpConcatBlock: 2-91               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-85        [2, 1024, 32, 32]         --\n",
       "│    └─UpConcatBlock: 2-106              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-107             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-87               [2, 64, 512, 512]         143,552\n",
       "│    └─UpConcatBlock: 2-108              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-87        [2, 64, 512, 512]         32,832\n",
       "│    └─UpConcatBlock: 2-91               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-88              [2, 1024, 32, 32]         --\n",
       "│    │    └─ConvBlock: 3-92              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-108              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-90              [2, 64, 512, 512]         110,720\n",
       "│    │    └─ConvBlock: 3-110             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-91               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-92              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-92               [2, 512, 64, 64]          --\n",
       "│    └─UpConcatBlock: 2-97               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-93        [2, 512, 64, 64]          --\n",
       "│    └─UpConcatBlock: 2-108              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-110             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-95               [2, 32, 1024, 1024]       35,936\n",
       "│    └─UpConcatBlock: 2-110              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-95        [2, 32, 1024, 1024]       8,224\n",
       "│    └─UpConcatBlock: 2-97               --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-96              [2, 512, 64, 64]          --\n",
       "│    │    └─ConvBlock: 3-97              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-98               [2, 256, 128, 128]        --\n",
       "│    └─UpConcatBlock: 2-104              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-98        [2, 256, 128, 128]        --\n",
       "│    └─UpConcatBlock: 2-110              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-99              [2, 32, 1024, 1024]       27,712\n",
       "│    │    └─ConvBlock: 3-113             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-104              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-101             [2, 256, 128, 128]        --\n",
       "│    │    └─ConvBlock: 3-104             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-110              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-113             --                        (recursive)\n",
       "│    └─Conv2d: 2-103                     [2, 1, 1024, 1024]        33\n",
       "│    └─UpConcatBlock: 2-104              --                        (recursive)\n",
       "│    │    └─ConvBlock: 3-104             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-105              [2, 128, 256, 256]        --\n",
       "│    └─UpConcatBlock: 2-106              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-105       [2, 128, 256, 256]        --\n",
       "│    │    └─ConvBlock: 3-106             [2, 128, 256, 256]        --\n",
       "│    │    └─ConvBlock: 3-107             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-107              [2, 64, 512, 512]         --\n",
       "│    └─UpConcatBlock: 2-108              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-108       [2, 64, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-109             [2, 64, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-110             --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-109              [2, 32, 1024, 1024]       --\n",
       "│    └─UpConcatBlock: 2-110              --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-111       [2, 32, 1024, 1024]       --\n",
       "│    │    └─ConvBlock: 3-112             [2, 32, 1024, 1024]       --\n",
       "│    │    └─ConvBlock: 3-113             --                        (recursive)\n",
       "│    └─Conv2d: 2-111                     [2, 1, 1024, 1024]        --\n",
       "==========================================================================================\n",
       "Total params: 1,644,715,076\n",
       "Trainable params: 1,644,715,076\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 750.95\n",
       "==========================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 5351.93\n",
       "Params size (MB): 1991.98\n",
       "Estimated Total Size (MB): 7394.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randn(batch_size, 3, input_size*2, input_size*2)\n",
    "summary(netD, input_data = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image_short, image_long, size):\n",
    "    H = image_short.shape[2]\n",
    "    W = image_short.shape[3]\n",
    "    ps = size\n",
    "    xx = np.random.randint(0, W - ps)\n",
    "    yy = np.random.randint(0, H - ps)\n",
    "    image_short = image_short[:,:,yy:yy + ps, xx:xx + ps]\n",
    "    image_long = image_long[:,:,yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2]\n",
    "    return image_short, image_long\n",
    "\n",
    "def pack_sony_raw(batch, device=None):\n",
    "    if not device:\n",
    "        device = idist.device()\n",
    "    batch = torch.maximum(batch - 512, torch.Tensor([0]).to(device=device)) / (16383 - 512)\n",
    "    H = batch.shape[2]\n",
    "    W = batch.shape[3]\n",
    "\n",
    "    out = torch.cat((batch[:,:, 0:H:2, 0:W:2], \n",
    "                     batch[:,:, 0:H:2, 1:W:2],\n",
    "                     batch[:,:, 1:H:2, 1:W:2],\n",
    "                     batch[:,:, 1:H:2, 0:W:2]), dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "def training_step(engine, batch):\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    short, long, ratio, cam_model, exposure_ratio, _, _, _ = batch\n",
    "\n",
    "    short = short.to(idist.device())\n",
    "    long = long.to(idist.device())\n",
    "\n",
    "    short = pack_sony_raw(short)\n",
    "\n",
    "    long = long / 65535.0\n",
    "    short = short * exposure_ratio.float().to(idist.device()).view(-1, 1, 1, 1)\n",
    "    short, long = random_crop(short, long, input_size)\n",
    "\n",
    "    # Train Discriminator with ground truth data\n",
    "    netD.zero_grad()\n",
    "    b_size = long.size(0)\n",
    "    label = torch.full((b_size,), real_label, dtype=torch.float, device=idist.device())\n",
    "\n",
    "    D_real_enc_out, D_real_dec_out = netD(long)\n",
    "    D_real_enc_out = D_real_enc_out.view(-1)\n",
    "    errD_real_enc = criterion(D_real_enc_out, label)\n",
    "    errD_real_dec = criterion(D_real_dec_out, label.view(-1, 1, 1, 1).expand_as(D_real_dec_out))\n",
    "    errD_real = errD_real_enc + errD_real_dec\n",
    "    errD_real.backward()\n",
    "\n",
    "    # Train with all-fake batch\n",
    "    fake = netG(short)\n",
    "    label.fill_(fake_label)\n",
    "\n",
    "    D_fake_enc_out, D_fake_dec_out = netD(fake.detach())\n",
    "    D_fake_enc_out = D_fake_enc_out.view(-1)\n",
    "    errD_fake_enc = criterion(D_fake_enc_out, label)\n",
    "    errD_fake_dec = criterion(D_fake_dec_out, label.view(-1, 1, 1, 1).expand_as(D_fake_dec_out))\n",
    "    errD_fake = errD_fake_enc + errD_fake_dec\n",
    "    errD_fake.backward()\n",
    "\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    # Train G\n",
    "    netG.zero_grad()\n",
    "    label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "    G_D_enc_out, G_D_dec_out = netD(fake)\n",
    "    \n",
    "    errG_l1 = loss(fake, long)\n",
    "    errG_dec = criterion(G_D_dec_out, label.view(-1, 1, 1, 1).expand_as(G_D_dec_out))\n",
    "    errG = 0.5*errG_l1 + 0.5*errG_dec\n",
    "    errG.backward()\n",
    "\n",
    "    optimizerG.step()\n",
    "\n",
    "    return {\n",
    "        \"Loss_G\" : errG.item(),\n",
    "        \"Loss_D\" : errD.item(),\n",
    "        \"D_real_enc\": errD_real_enc.mean().item(),\n",
    "        \"D_real_dec\": errD_real_dec.mean().item(),\n",
    "        \"D_fake_enc\": errD_fake_enc.mean().item(),\n",
    "        \"D_fake_dec\": errD_fake_dec.mean().item(),\n",
    "        \"D_G_L1\": errG_l1.item(),\n",
    "        \"D_G_dec\": errG_dec.mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pikachu/.local/lib/python3.10/site-packages/ignite/contrib/handlers/tqdm_logger.py:127: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "trainer = Engine(training_step)\n",
    "losses_key = [\"Loss_G\",\"Loss_D\",\"D_real_enc\",\"D_real_dec\",\"D_fake_enc\",\"D_fake_dec\",\"D_G_L1\",\"D_G_dec\"]\n",
    "losses = {}\n",
    "for k in losses_key:\n",
    "    losses[k] = []\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_G\"]).attach(trainer, \"Loss_G\")\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_D\"]).attach(trainer, \"Loss_D\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_real_enc\"]).attach(trainer, \"D_real_enc\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_real_dec\"]).attach(trainer, \"D_real_dec\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_fake_enc\"]).attach(trainer, \"D_fake_enc\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_fake_dec\"]).attach(trainer, \"D_fake_dec\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_L1\"]).attach(trainer, \"D_G_L1\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_dec\"]).attach(trainer, \"D_G_dec\")  \n",
    "ProgressBar().attach(trainer, metric_names=[\"Loss_G\",\"Loss_D\",\"D_real_enc\",\"D_real_dec\",\"D_fake_enc\",\"D_fake_dec\",\"D_G_L1\",\"D_G_dec\"])\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_losses(engine):\n",
    "    o = engine.state.output\n",
    "    print(o[\"Loss_G\"])\n",
    "    for k in losses_key:\n",
    "        losses[k].append(o[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 09:58:14,160 ignite.distributed.launcher.Parallel INFO: Initialized processing group with backend: 'nccl'\n",
      "2023-06-10 09:58:14,162 ignite.distributed.launcher.Parallel INFO: - Run '<function training at 0x7f3137fa8c10>' in 1 processes\n",
      "Epoch [1/100]: [388/466]  83%|████████▎ , Loss_G=0.401, Loss_D=2.84, D_real_enc=0.722, D_real_dec=0.71, D_fake_enc=0.732, D_fake_dec=0.676, D_G_L1=0.0901, D_G_dec=0.713 [20:19<04:08] "
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "def training(*args):\n",
    "    trainer.run(sony_dataloader, max_epochs=num_epoch)\n",
    "\n",
    "with idist.Parallel(backend='nccl') as parallel:\n",
    "    parallel.run(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epoch,\n",
    "            'model_state_dict': netG.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'modelD_state_dict': netD.state_dict(),\n",
    "            'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "            }, 'model_seed_{}.pt'.format(torch.random.initial_seed()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
