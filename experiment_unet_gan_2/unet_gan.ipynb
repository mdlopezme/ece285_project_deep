{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f697bf16cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import rawpy\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import LabeledDataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import ignite.distributed as idist\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.metrics import FID, InceptionScore, RunningAverage\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(9706507364633013713)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 00:24:31,501 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<utils.datasets.Labe': \n",
      "\t{'batch_size': 8, 'num_workers': 8, 'shuffle': True, 'drop_last': True, 'prefetch_factor': 1, 'pin_memory': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 512])\n",
      "torch.Size([3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"dataset\"\n",
    "sony_csv_files = [\"dataset/Sony_train_list.txt\"]\n",
    "fuji_csv_files =  [\"dataset/Fuji_train_list.txt\"]\n",
    "\n",
    "batch_size = 8\n",
    "input_size = 512\n",
    "\n",
    "pre_crop_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "sony_dataset = LabeledDataset(root_dir, *sony_csv_files, transform=pre_crop_transform, training=True, crop_size=input_size)\n",
    "sony_dataloader = idist.auto_dataloader(sony_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True, prefetch_factor=1)\n",
    "print(sony_dataset[0][0].shape)\n",
    "print(sony_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sony_dataset.prime_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unet.unet_model import UNet\n",
    "from torch import optim \n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.lrelu1_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel, stride=stride,  padding=padding)\n",
    "        self.lrelu1_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.lrelu1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.lrelu1_2(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlockBN(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel=3, stride=1, padding=1):\n",
    "        super(ConvBlockBN, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.batchnorm1_1 = nn.BatchNorm2d(out_channel)\n",
    "        self.lrelu1_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel, stride=stride,  padding=padding)\n",
    "        self.batchnorm1_2 = nn.BatchNorm2d(out_channel)\n",
    "        self.lrelu1_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.batchnorm1_1(x)\n",
    "        x = self.lrelu1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.batchnorm1_2(x)\n",
    "        x = self.lrelu1_2(x)\n",
    "        return x\n",
    "    \n",
    "class UpConcatBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, conv_block):\n",
    "        super(UpConcatBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channel, in_channel // 2, kernel_size=2, stride=2)\n",
    "        self.conv_block = conv_block(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.deconv(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.down1 = ConvBlock(in_feat, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = ConvBlock(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = ConvBlock(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.down5 = ConvBlock(256, 512)\n",
    "\n",
    "        self.up5 = UpConcatBlock(512, 256, ConvBlock)\n",
    "        self.up4 = UpConcatBlock(256, 128, ConvBlock)\n",
    "        self.up3 = UpConcatBlock(128, 64, ConvBlock)\n",
    "        self.up2 = UpConcatBlock(64, 32, ConvBlock)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, out_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(self.pool1(down1))\n",
    "        down3 = self.down3(self.pool2(down2))\n",
    "        down4 = self.down4(self.pool3(down3))\n",
    "        down5 = self.down5(self.pool4(down4))\n",
    "\n",
    "        up = self.up5(down5, down4)\n",
    "        up = self.up4(up, down3)\n",
    "        up = self.up3(up, down2)\n",
    "        up = self.up2(up, down1)\n",
    "\n",
    "        out = self.conv10(up)\n",
    "        out = torch.nn.functional.pixel_shuffle(out, 2)\n",
    "        return out\n",
    "    \n",
    "class UNet_D(nn.Module):\n",
    "    def __init__(self, in_feat):\n",
    "        super(UNet_D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_feat, 16, 4, 2, 1)\n",
    "\n",
    "        self.down1 = ConvBlockBN(16, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = ConvBlockBN(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = ConvBlockBN(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = ConvBlockBN(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.down5 = ConvBlockBN(256, 512)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "        self.down6 = ConvBlockBN(512, 1024)\n",
    "        self.pool6 = nn.MaxPool2d(2)\n",
    "        self.down7 = ConvBlockBN(1024, 2048)\n",
    "        self.pool7 = nn.MaxPool2d(2)\n",
    "        self.down8 = ConvBlockBN(2048, 4096)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096*4*4, 1)\n",
    "\n",
    "        self.up8 = UpConcatBlock(4096, 2048, ConvBlockBN)\n",
    "        self.up7 = UpConcatBlock(2048, 1024, ConvBlockBN)\n",
    "        self.up6 = UpConcatBlock(1024, 512, ConvBlockBN)\n",
    "        self.up5 = UpConcatBlock(512, 256, ConvBlockBN)\n",
    "        self.up4 = UpConcatBlock(256, 128, ConvBlockBN)\n",
    "        self.up3 = UpConcatBlock(128, 64, ConvBlockBN)\n",
    "        self.up2 = UpConcatBlock(64, 32, ConvBlockBN)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(self.pool1(down1))\n",
    "        down3 = self.down3(self.pool2(down2))\n",
    "        down4 = self.down4(self.pool3(down3))\n",
    "        down5 = self.down5(self.pool4(down4))\n",
    "        down6 = self.down6(self.pool5(down5))\n",
    "        down7 = self.down7(self.pool6(down6))\n",
    "        down8 = self.down8(self.pool7(down7))\n",
    "\n",
    "        down8_ = torch.flatten(down8, 1)\n",
    "        real_fake = self.fc1(down8_)\n",
    "\n",
    "        up = self.up8(down8, down7)\n",
    "        up = self.up7(up, down6)\n",
    "        up = self.up6(up, down5)\n",
    "        up = self.up5(up, down4)\n",
    "        up = self.up4(up, down3)\n",
    "        up = self.up3(up, down2)\n",
    "        up = self.up2(up, down1)\n",
    "\n",
    "        out = self.conv10(up)\n",
    "        return real_fake, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 00:25:33,245 ignite.distributed.auto.auto_model INFO: Apply torch DataParallel on model\n",
      "2023-06-12 00:25:35,813 ignite.distributed.auto.auto_model INFO: Apply torch DataParallel on model\n"
     ]
    }
   ],
   "source": [
    "netG = idist.auto_model(UNet(4, 12))\n",
    "netD = idist.auto_model(UNet_D(3))\n",
    "optimizerG = idist.auto_optim(optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.999)))\n",
    "optimizerD = idist.auto_optim(optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.999)))\n",
    "loss = nn.L1Loss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "lr_scheduler = LRScheduler(optim.lr_scheduler.StepLR(optimizerG, step_size=10, gamma=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DataParallel                             [8, 3, 1024, 1024]        --\n",
       "├─UNet: 1-1                              [4, 3, 1024, 1024]        7,760,748\n",
       "├─UNet: 1-4                              --                        (recursive)\n",
       "│    └─ConvBlock: 2-1                    [4, 32, 512, 512]         10,432\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-1                  [4, 32, 512, 512]         1,184\n",
       "├─UNet: 1-3                              [4, 3, 1024, 1024]        --\n",
       "├─UNet: 1-4                              --                        (recursive)\n",
       "│    └─ConvBlock: 2-3                    [4, 32, 512, 512]         --\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-2                  [4, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-3               [4, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-4                  [4, 32, 512, 512]         9,248\n",
       "│    │    └─LeakyReLU: 3-5               [4, 32, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-5                    [4, 32, 256, 256]         --\n",
       "│    └─ConvBlock: 2-6                    [4, 64, 256, 256]         55,424\n",
       "│    └─ConvBlock: 2-29                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-6                  [4, 64, 256, 256]         18,496\n",
       "│    │    └─LeakyReLU: 3-7               [4, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-8                  [4, 64, 256, 256]         36,928\n",
       "│    │    └─LeakyReLU: 3-9               [4, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-8                    [4, 64, 128, 128]         --\n",
       "│    └─ConvBlock: 2-9                    [4, 128, 128, 128]        221,440\n",
       "│    └─ConvBlock: 2-32                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-10                 [4, 128, 128, 128]        73,856\n",
       "│    │    └─LeakyReLU: 3-11              [4, 128, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-12                 [4, 128, 128, 128]        147,584\n",
       "│    │    └─LeakyReLU: 3-13              [4, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-11                   [4, 128, 64, 64]          --\n",
       "│    └─ConvBlock: 2-12                   [4, 256, 64, 64]          885,248\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-14                 [4, 256, 64, 64]          295,168\n",
       "│    │    └─LeakyReLU: 3-15              [4, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-16                 [4, 256, 64, 64]          590,080\n",
       "│    │    └─LeakyReLU: 3-17              [4, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-14                   [4, 256, 32, 32]          --\n",
       "│    └─ConvBlock: 2-15                   [4, 512, 32, 32]          3,539,968\n",
       "│    └─ConvBlock: 2-38                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-18                 [4, 512, 32, 32]          1,180,160\n",
       "│    │    └─LeakyReLU: 3-19              [4, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-20                 [4, 512, 32, 32]          2,359,808\n",
       "│    │    └─LeakyReLU: 3-21              [4, 512, 32, 32]          --\n",
       "│    └─UpConcatBlock: 2-17               [4, 256, 64, 64]          2,294,528\n",
       "│    └─UpConcatBlock: 2-40               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-22        [4, 256, 64, 64]          524,544\n",
       "│    │    └─ConvBlock: 3-23              [4, 256, 64, 64]          1,769,984\n",
       "│    │    └─ConvBlock: 3-55              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-19               [4, 128, 128, 128]        573,824\n",
       "│    └─UpConcatBlock: 2-42               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-25        [4, 128, 128, 128]        131,200\n",
       "│    │    └─ConvBlock: 3-26              [4, 128, 128, 128]        442,624\n",
       "│    │    └─ConvBlock: 3-58              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-21               [4, 64, 256, 256]         143,552\n",
       "│    └─UpConcatBlock: 2-44               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-28        [4, 64, 256, 256]         32,832\n",
       "│    │    └─ConvBlock: 3-29              [4, 64, 256, 256]         110,720\n",
       "│    │    └─ConvBlock: 3-61              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-23               [4, 32, 512, 512]         35,936\n",
       "│    └─UpConcatBlock: 2-46               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-31        [4, 32, 512, 512]         8,224\n",
       "│    │    └─ConvBlock: 3-32              [4, 32, 512, 512]         27,712\n",
       "│    │    └─ConvBlock: 3-64              --                        (recursive)\n",
       "│    └─Conv2d: 2-25                      [4, 12, 512, 512]         396\n",
       "│    └─ConvBlock: 2-26                   --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-34              [4, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-35                 [4, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-36              [4, 32, 512, 512]         --\n",
       "│    └─MaxPool2d: 2-27                   [4, 32, 256, 256]         --\n",
       "│    └─ConvBlock: 2-28                   [4, 64, 256, 256]         --\n",
       "│    └─ConvBlock: 2-29                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-37                 [4, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-38              [4, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-39                 [4, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-40              [4, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-30                   [4, 64, 128, 128]         --\n",
       "│    └─ConvBlock: 2-31                   [4, 128, 128, 128]        --\n",
       "│    └─ConvBlock: 2-32                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-41                 [4, 128, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-42              [4, 128, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-43                 [4, 128, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-44              [4, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-33                   [4, 128, 64, 64]          --\n",
       "│    └─ConvBlock: 2-34                   [4, 256, 64, 64]          --\n",
       "│    └─ConvBlock: 2-35                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-45                 [4, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-46              [4, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-47                 [4, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-48              [4, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-36                   [4, 256, 32, 32]          --\n",
       "│    └─ConvBlock: 2-37                   [4, 512, 32, 32]          --\n",
       "│    └─ConvBlock: 2-38                   --                        (recursive)\n",
       "│    │    └─Conv2d: 3-49                 [4, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-50              [4, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-51                 [4, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-52              [4, 512, 32, 32]          --\n",
       "│    └─UpConcatBlock: 2-39               [4, 256, 64, 64]          --\n",
       "│    └─UpConcatBlock: 2-40               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-53        [4, 256, 64, 64]          --\n",
       "│    │    └─ConvBlock: 3-54              [4, 256, 64, 64]          --\n",
       "│    │    └─ConvBlock: 3-55              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-41               [4, 128, 128, 128]        --\n",
       "│    └─UpConcatBlock: 2-42               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-56        [4, 128, 128, 128]        --\n",
       "│    │    └─ConvBlock: 3-57              [4, 128, 128, 128]        --\n",
       "│    │    └─ConvBlock: 3-58              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-43               [4, 64, 256, 256]         --\n",
       "│    └─UpConcatBlock: 2-44               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-59        [4, 64, 256, 256]         --\n",
       "│    │    └─ConvBlock: 3-60              [4, 64, 256, 256]         --\n",
       "│    │    └─ConvBlock: 3-61              --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-45               [4, 32, 512, 512]         --\n",
       "│    └─UpConcatBlock: 2-46               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-62        [4, 32, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-63              [4, 32, 512, 512]         --\n",
       "│    │    └─ConvBlock: 3-64              --                        (recursive)\n",
       "│    └─Conv2d: 2-47                      [4, 12, 512, 512]         --\n",
       "==========================================================================================\n",
       "Total params: 25,632,888\n",
       "Trainable params: 25,632,888\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 219.91\n",
       "==========================================================================================\n",
       "Input size (MB): 33.55\n",
       "Forward/backward pass size (MB): 2650.80\n",
       "Params size (MB): 31.04\n",
       "Estimated Total Size (MB): 2715.40\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randn(batch_size, 4, input_size, input_size)\n",
    "summary(netG, input_data = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DataParallel                             [8, 1]                    --\n",
       "├─UNet_D: 1-1                            [4, 1]                    497,851,282\n",
       "├─UNet_D: 1-4                            --                        (recursive)\n",
       "│    └─Conv2d: 2-1                       [4, 16, 512, 512]         784\n",
       "├─UNet_D: 1-3                            [4, 1]                    --\n",
       "├─UNet_D: 1-4                            --                        (recursive)\n",
       "│    └─Conv2d: 2-2                       [4, 16, 512, 512]         --\n",
       "│    └─ConvBlockBN: 2-3                  [4, 32, 512, 512]         14,016\n",
       "│    └─ConvBlockBN: 2-8                  --                        (recursive)\n",
       "│    │    └─Conv2d: 3-1                  [4, 32, 512, 512]         4,640\n",
       "│    └─ConvBlockBN: 2-5                  [4, 32, 512, 512]         --\n",
       "│    └─ConvBlockBN: 2-8                  --                        (recursive)\n",
       "│    │    └─Conv2d: 3-2                  [4, 32, 512, 512]         --\n",
       "│    │    └─BatchNorm2d: 3-3             [4, 32, 512, 512]         --\n",
       "│    │    └─BatchNorm2d: 3-4             [4, 32, 512, 512]         64\n",
       "│    │    └─LeakyReLU: 3-5               [4, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-6               [4, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-7                  [4, 32, 512, 512]         --\n",
       "│    │    └─Conv2d: 3-8                  [4, 32, 512, 512]         9,248\n",
       "│    │    └─BatchNorm2d: 3-9             [4, 32, 512, 512]         --\n",
       "│    │    └─LeakyReLU: 3-10              [4, 32, 512, 512]         --\n",
       "│    │    └─BatchNorm2d: 3-11            [4, 32, 512, 512]         64\n",
       "│    └─MaxPool2d: 2-7                    [4, 32, 256, 256]         --\n",
       "│    └─ConvBlockBN: 2-8                  --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-12              [4, 32, 512, 512]         --\n",
       "│    └─ConvBlockBN: 2-9                  [4, 64, 256, 256]         --\n",
       "│    └─ConvBlockBN: 2-16                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-13                 [4, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-11                   [4, 32, 256, 256]         --\n",
       "│    └─ConvBlockBN: 2-16                 --                        (recursive)\n",
       "│    │    └─BatchNorm2d: 3-14            [4, 64, 256, 256]         --\n",
       "│    └─ConvBlockBN: 2-13                 [4, 64, 256, 256]         55,680\n",
       "│    └─ConvBlockBN: 2-16                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-15                 [4, 64, 256, 256]         18,496\n",
       "│    │    └─LeakyReLU: 3-16              [4, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-17                 [4, 64, 256, 256]         --\n",
       "│    │    └─BatchNorm2d: 3-18            [4, 64, 256, 256]         128\n",
       "│    │    └─BatchNorm2d: 3-19            [4, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-20              [4, 64, 256, 256]         --\n",
       "│    │    └─LeakyReLU: 3-21              [4, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-22                 [4, 64, 256, 256]         36,928\n",
       "│    └─MaxPool2d: 2-15                   [4, 64, 128, 128]         --\n",
       "│    └─ConvBlockBN: 2-16                 --                        (recursive)\n",
       "│    │    └─BatchNorm2d: 3-23            [4, 64, 256, 256]         128\n",
       "│    │    └─LeakyReLU: 3-24              [4, 64, 256, 256]         --\n",
       "│    └─ConvBlockBN: 2-17                 [4, 128, 128, 128]        --\n",
       "│    └─ConvBlockBN: 2-22                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-25                 [4, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-19                   [4, 64, 128, 128]         --\n",
       "│    └─ConvBlockBN: 2-22                 --                        (recursive)\n",
       "│    │    └─BatchNorm2d: 3-26            [4, 128, 128, 128]        --\n",
       "│    └─ConvBlockBN: 2-21                 [4, 128, 128, 128]        221,952\n",
       "│    └─ConvBlockBN: 2-22                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-27                 [4, 128, 128, 128]        73,856\n",
       "│    │    └─LeakyReLU: 3-28              [4, 128, 128, 128]        --\n",
       "│    │    └─BatchNorm2d: 3-29            [4, 128, 128, 128]        256\n",
       "│    │    └─Conv2d: 3-30                 [4, 128, 128, 128]        --\n",
       "│    │    └─LeakyReLU: 3-31              [4, 128, 128, 128]        --\n",
       "│    │    └─BatchNorm2d: 3-32            [4, 128, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-33                 [4, 128, 128, 128]        147,584\n",
       "│    │    └─LeakyReLU: 3-34              [4, 128, 128, 128]        --\n",
       "│    │    └─BatchNorm2d: 3-35            [4, 128, 128, 128]        256\n",
       "│    │    └─LeakyReLU: 3-36              [4, 128, 128, 128]        --\n",
       "│    └─MaxPool2d: 2-23                   [4, 128, 64, 64]          --\n",
       "│    └─ConvBlockBN: 2-24                 [4, 256, 64, 64]          --\n",
       "│    └─ConvBlockBN: 2-28                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-37                 [4, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-26                   [4, 128, 64, 64]          --\n",
       "│    └─ConvBlockBN: 2-27                 [4, 256, 64, 64]          886,272\n",
       "│    └─ConvBlockBN: 2-28                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-38                 [4, 256, 64, 64]          295,168\n",
       "│    │    └─BatchNorm2d: 3-39            [4, 256, 64, 64]          --\n",
       "│    │    └─BatchNorm2d: 3-40            [4, 256, 64, 64]          512\n",
       "│    │    └─LeakyReLU: 3-41              [4, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-42              [4, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-43                 [4, 256, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-44                 [4, 256, 64, 64]          590,080\n",
       "│    │    └─BatchNorm2d: 3-45            [4, 256, 64, 64]          --\n",
       "│    │    └─BatchNorm2d: 3-46            [4, 256, 64, 64]          512\n",
       "│    │    └─LeakyReLU: 3-47              [4, 256, 64, 64]          --\n",
       "│    │    └─LeakyReLU: 3-48              [4, 256, 64, 64]          --\n",
       "│    └─MaxPool2d: 2-29                   [4, 256, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-30                   [4, 256, 32, 32]          --\n",
       "│    └─ConvBlockBN: 2-31                 [4, 512, 32, 32]          --\n",
       "│    └─ConvBlockBN: 2-36                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-49                 [4, 512, 32, 32]          --\n",
       "│    └─ConvBlockBN: 2-33                 [4, 512, 32, 32]          3,542,016\n",
       "│    └─ConvBlockBN: 2-36                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-50                 [4, 512, 32, 32]          1,180,160\n",
       "│    │    └─BatchNorm2d: 3-51            [4, 512, 32, 32]          --\n",
       "│    │    └─BatchNorm2d: 3-52            [4, 512, 32, 32]          1,024\n",
       "│    │    └─LeakyReLU: 3-53              [4, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-54              [4, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-55                 [4, 512, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-56                 [4, 512, 32, 32]          2,359,808\n",
       "│    │    └─BatchNorm2d: 3-57            [4, 512, 32, 32]          --\n",
       "│    │    └─LeakyReLU: 3-58              [4, 512, 32, 32]          --\n",
       "│    │    └─BatchNorm2d: 3-59            [4, 512, 32, 32]          1,024\n",
       "│    └─MaxPool2d: 2-35                   [4, 512, 16, 16]          --\n",
       "│    └─ConvBlockBN: 2-36                 --                        (recursive)\n",
       "│    │    └─LeakyReLU: 3-60              [4, 512, 32, 32]          --\n",
       "│    └─ConvBlockBN: 2-37                 [4, 1024, 16, 16]         --\n",
       "│    └─ConvBlockBN: 2-41                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-61                 [4, 1024, 16, 16]         --\n",
       "│    └─MaxPool2d: 2-39                   [4, 512, 16, 16]          --\n",
       "│    └─ConvBlockBN: 2-40                 [4, 1024, 16, 16]         14,161,920\n",
       "│    └─ConvBlockBN: 2-41                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-62                 [4, 1024, 16, 16]         4,719,616\n",
       "│    │    └─BatchNorm2d: 3-63            [4, 1024, 16, 16]         --\n",
       "│    │    └─BatchNorm2d: 3-64            [4, 1024, 16, 16]         2,048\n",
       "│    │    └─LeakyReLU: 3-65              [4, 1024, 16, 16]         --\n",
       "│    │    └─LeakyReLU: 3-66              [4, 1024, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-67                 [4, 1024, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-68                 [4, 1024, 16, 16]         9,438,208\n",
       "│    │    └─BatchNorm2d: 3-69            [4, 1024, 16, 16]         --\n",
       "│    │    └─BatchNorm2d: 3-70            [4, 1024, 16, 16]         2,048\n",
       "│    │    └─LeakyReLU: 3-71              [4, 1024, 16, 16]         --\n",
       "│    │    └─LeakyReLU: 3-72              [4, 1024, 16, 16]         --\n",
       "│    └─MaxPool2d: 2-42                   [4, 1024, 8, 8]           --\n",
       "│    └─MaxPool2d: 2-43                   [4, 1024, 8, 8]           --\n",
       "│    └─ConvBlockBN: 2-44                 [4, 2048, 8, 8]           --\n",
       "│    └─ConvBlockBN: 2-51                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-73                 [4, 2048, 8, 8]           --\n",
       "│    └─ConvBlockBN: 2-46                 [4, 2048, 8, 8]           56,635,392\n",
       "│    └─ConvBlockBN: 2-51                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-74                 [4, 2048, 8, 8]           18,876,416\n",
       "│    │    └─BatchNorm2d: 3-75            [4, 2048, 8, 8]           4,096\n",
       "│    │    └─BatchNorm2d: 3-76            [4, 2048, 8, 8]           --\n",
       "│    │    └─LeakyReLU: 3-77              [4, 2048, 8, 8]           --\n",
       "│    │    └─LeakyReLU: 3-78              [4, 2048, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-79                 [4, 2048, 8, 8]           37,750,784\n",
       "│    │    └─Conv2d: 3-80                 [4, 2048, 8, 8]           --\n",
       "│    │    └─BatchNorm2d: 3-81            [4, 2048, 8, 8]           4,096\n",
       "│    │    └─LeakyReLU: 3-82              [4, 2048, 8, 8]           --\n",
       "│    └─MaxPool2d: 2-48                   [4, 2048, 4, 4]           --\n",
       "│    └─ConvBlockBN: 2-49                 [4, 4096, 4, 4]           226,516,992\n",
       "│    └─ConvBlockBN: 2-54                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-83                 [4, 4096, 4, 4]           75,501,568\n",
       "│    └─ConvBlockBN: 2-51                 --                        (recursive)\n",
       "│    │    └─BatchNorm2d: 3-84            [4, 2048, 8, 8]           --\n",
       "│    │    └─LeakyReLU: 3-85              [4, 2048, 8, 8]           --\n",
       "│    └─MaxPool2d: 2-52                   [4, 2048, 4, 4]           --\n",
       "│    └─ConvBlockBN: 2-53                 [4, 4096, 4, 4]           --\n",
       "│    └─ConvBlockBN: 2-54                 --                        (recursive)\n",
       "│    │    └─Conv2d: 3-86                 [4, 4096, 4, 4]           --\n",
       "│    │    └─BatchNorm2d: 3-87            [4, 4096, 4, 4]           8,192\n",
       "│    │    └─BatchNorm2d: 3-88            [4, 4096, 4, 4]           --\n",
       "│    │    └─LeakyReLU: 3-89              [4, 4096, 4, 4]           --\n",
       "│    │    └─LeakyReLU: 3-90              [4, 4096, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-91                 [4, 4096, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-92                 [4, 4096, 4, 4]           150,999,040\n",
       "│    │    └─BatchNorm2d: 3-93            [4, 4096, 4, 4]           8,192\n",
       "│    │    └─BatchNorm2d: 3-94            [4, 4096, 4, 4]           --\n",
       "│    │    └─LeakyReLU: 3-95              [4, 4096, 4, 4]           --\n",
       "│    │    └─LeakyReLU: 3-96              [4, 4096, 4, 4]           --\n",
       "│    └─Linear: 2-55                      [4, 1]                    65,537\n",
       "│    └─Linear: 2-56                      [4, 1]                    --\n",
       "│    └─UpConcatBlock: 2-57               [4, 2048, 8, 8]           --\n",
       "│    └─UpConcatBlock: 2-60               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-97        [4, 2048, 8, 8]           --\n",
       "│    └─UpConcatBlock: 2-59               [4, 2048, 8, 8]           146,814,976\n",
       "│    └─UpConcatBlock: 2-60               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-98        [4, 2048, 8, 8]           33,556,480\n",
       "│    │    └─ConvBlockBN: 3-99            [4, 2048, 8, 8]           --\n",
       "│    │    └─ConvBlockBN: 3-102           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-101           [4, 2048, 8, 8]           113,258,496\n",
       "│    │    └─ConvBlockBN: 3-102           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-61               [4, 1024, 16, 16]         36,707,328\n",
       "│    └─UpConcatBlock: 2-64               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-103       [4, 1024, 16, 16]         8,389,632\n",
       "│    └─UpConcatBlock: 2-63               [4, 1024, 16, 16]         --\n",
       "│    └─UpConcatBlock: 2-64               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-104       [4, 1024, 16, 16]         --\n",
       "│    │    └─ConvBlockBN: 3-105           [4, 1024, 16, 16]         28,317,696\n",
       "│    │    └─ConvBlockBN: 3-108           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-107           [4, 1024, 16, 16]         --\n",
       "│    │    └─ConvBlockBN: 3-108           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-65               [4, 512, 32, 32]          9,178,624\n",
       "│    └─UpConcatBlock: 2-71               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-109       [4, 512, 32, 32]          2,097,664\n",
       "│    └─UpConcatBlock: 2-67               [4, 512, 32, 32]          --\n",
       "│    └─UpConcatBlock: 2-71               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-110       [4, 512, 32, 32]          --\n",
       "│    │    └─ConvBlockBN: 3-111           [4, 512, 32, 32]          7,080,960\n",
       "│    │    └─ConvBlockBN: 3-116           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-113           [4, 512, 32, 32]          --\n",
       "│    │    └─ConvBlockBN: 3-116           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-69               [4, 256, 64, 64]          2,295,552\n",
       "│    └─UpConcatBlock: 2-73               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-115       [4, 256, 64, 64]          524,544\n",
       "│    └─UpConcatBlock: 2-71               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-116           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-72               [4, 256, 64, 64]          --\n",
       "│    └─UpConcatBlock: 2-73               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-117       [4, 256, 64, 64]          --\n",
       "│    │    └─ConvBlockBN: 3-118           [4, 256, 64, 64]          1,771,008\n",
       "│    │    └─ConvBlockBN: 3-121           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-120           [4, 256, 64, 64]          --\n",
       "│    │    └─ConvBlockBN: 3-121           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-74               [4, 128, 128, 128]        574,336\n",
       "│    └─UpConcatBlock: 2-80               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-122       [4, 128, 128, 128]        131,200\n",
       "│    └─UpConcatBlock: 2-76               [4, 128, 128, 128]        --\n",
       "│    └─UpConcatBlock: 2-80               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-123       [4, 128, 128, 128]        --\n",
       "│    │    └─ConvBlockBN: 3-124           [4, 128, 128, 128]        443,136\n",
       "│    │    └─ConvBlockBN: 3-129           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-126           [4, 128, 128, 128]        --\n",
       "│    │    └─ConvBlockBN: 3-129           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-78               [4, 64, 256, 256]         143,808\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-128       [4, 64, 256, 256]         32,832\n",
       "│    └─UpConcatBlock: 2-80               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-129           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-130           [4, 64, 256, 256]         110,976\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-82               [4, 64, 256, 256]         --\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-132       [4, 64, 256, 256]         --\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-134           [4, 64, 256, 256]         --\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-84               [4, 32, 512, 512]         36,064\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-136       [4, 32, 512, 512]         8,224\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-138           [4, 32, 512, 512]         27,840\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-90               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-142           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    └─UpConcatBlock: 2-92               [4, 32, 512, 512]         --\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvTranspose2d: 3-144       [4, 32, 512, 512]         --\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-146           [4, 32, 512, 512]         --\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    └─Conv2d: 2-94                      [4, 1, 512, 512]          33\n",
       "│    └─UpConcatBlock: 2-95               --                        (recursive)\n",
       "│    │    └─ConvBlockBN: 3-148           --                        (recursive)\n",
       "│    └─Conv2d: 2-96                      [4, 1, 512, 512]          --\n",
       "==========================================================================================\n",
       "Total params: 1,644,497,604\n",
       "Trainable params: 1,644,497,604\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 380.22\n",
       "==========================================================================================\n",
       "Input size (MB): 100.66\n",
       "Forward/backward pass size (MB): 4945.08\n",
       "Params size (MB): 1991.41\n",
       "Estimated Total Size (MB): 7037.15\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.randn(batch_size, 3, input_size*2, input_size*2)\n",
    "summary(netD, input_data = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image_short, image_long, size):\n",
    "    H = image_short.shape[2]\n",
    "    W = image_short.shape[3]\n",
    "    ps = size\n",
    "    xx = np.random.randint(0, W - ps)\n",
    "    yy = np.random.randint(0, H - ps)\n",
    "    image_short = image_short[:,:,yy:yy + ps, xx:xx + ps]\n",
    "    image_long = image_long[:,:,yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2]\n",
    "    return image_short, image_long\n",
    "\n",
    "def pack_sony_raw(batch, device=None):\n",
    "    if not device:\n",
    "        device = idist.device()\n",
    "    batch = torch.maximum(batch - 512, torch.Tensor([0]).to(device=device)) / (16383 - 512)\n",
    "    H = batch.shape[2]\n",
    "    W = batch.shape[3]\n",
    "\n",
    "    out = torch.cat((batch[:,:, 0:H:2, 0:W:2], \n",
    "                     batch[:,:, 0:H:2, 1:W:2],\n",
    "                     batch[:,:, 1:H:2, 1:W:2],\n",
    "                     batch[:,:, 1:H:2, 0:W:2]), dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "def training_step(engine, batch):\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    short, long, ratio, cam_model, exposure_ratio, _, _= batch\n",
    "\n",
    "    short = short.to(idist.device())\n",
    "    long = long.to(idist.device())\n",
    "\n",
    "    # short = pack_sony_raw(short)\n",
    "\n",
    "    long = long / 65535.0\n",
    "    short = short * exposure_ratio.float().to(idist.device()).view(-1, 1, 1, 1)\n",
    "    # short, long = random_crop(short, long, input_size)\n",
    "\n",
    "    # Train Discriminator with ground truth data\n",
    "    netD.zero_grad()\n",
    "    b_size = long.size(0)\n",
    "    label = torch.full((b_size,), real_label, dtype=torch.float, device=idist.device())\n",
    "\n",
    "    D_real_enc_out, D_real_dec_out = netD(long)\n",
    "    D_real_enc_out = D_real_enc_out.view(-1)\n",
    "    errD_real_enc = criterion(D_real_enc_out, label)\n",
    "    errD_real_dec = criterion(D_real_dec_out, label.view(-1, 1, 1, 1).expand_as(D_real_dec_out))\n",
    "    errD_real = errD_real_enc + errD_real_dec\n",
    "    errD_real.backward()\n",
    "\n",
    "    # Train with all-fake batch\n",
    "    fake = netG(short)\n",
    "    label.fill_(fake_label)\n",
    "\n",
    "    D_fake_enc_out, D_fake_dec_out = netD(fake.detach())\n",
    "    D_fake_enc_out = D_fake_enc_out.view(-1)\n",
    "    errD_fake_enc = criterion(D_fake_enc_out, label)\n",
    "    errD_fake_dec = criterion(D_fake_dec_out, label.view(-1, 1, 1, 1).expand_as(D_fake_dec_out))\n",
    "    errD_fake = errD_fake_enc + errD_fake_dec\n",
    "    errD_fake.backward()\n",
    "\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    # Train G\n",
    "    netG.zero_grad()\n",
    "    label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "    G_D_enc_out, G_D_dec_out = netD(fake)\n",
    "    \n",
    "    errG_l1 = loss(fake, long)\n",
    "    errG_dec = criterion(G_D_dec_out, label.view(-1, 1, 1, 1).expand_as(G_D_dec_out))\n",
    "    errG = 0.999*errG_l1 + 0.001*errG_dec   # Target loss: 0.02, errG_dec (10%) at ~3 => 0.001*3\n",
    "    errG.backward()\n",
    "\n",
    "    optimizerG.step()\n",
    "\n",
    "    return {\n",
    "        \"Loss_G\" : errG.item(),\n",
    "        \"Loss_D\" : errD.item(),\n",
    "        \"D_real_enc\": errD_real_enc.mean().item(),\n",
    "        \"D_real_dec\": errD_real_dec.mean().item(),\n",
    "        \"D_fake_enc\": errD_fake_enc.mean().item(),\n",
    "        \"D_fake_dec\": errD_fake_dec.mean().item(),\n",
    "        \"D_G_L1\": errG_l1.item(),\n",
    "        \"D_G_dec\": errG_dec.mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pikachu/.local/lib/python3.10/site-packages/ignite/contrib/handlers/tqdm_logger.py:127: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "trainer = Engine(training_step)\n",
    "trainer.add_event_handler(Events.EPOCH_STARTED, lr_scheduler)\n",
    "losses_key = [\"Loss_G\",\"Loss_D\",\"D_real_enc\",\"D_real_dec\",\"D_fake_enc\",\"D_fake_dec\",\"D_G_L1\",\"D_G_dec\"]\n",
    "losses = {}\n",
    "for k in losses_key:\n",
    "    losses[k] = []\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_G\"]).attach(trainer, \"Loss_G\")\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_D\"]).attach(trainer, \"Loss_D\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_real_enc\"]).attach(trainer, \"D_real_enc\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_real_dec\"]).attach(trainer, \"D_real_dec\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_fake_enc\"]).attach(trainer, \"D_fake_enc\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_fake_dec\"]).attach(trainer, \"D_fake_dec\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_L1\"]).attach(trainer, \"D_G_L1\")  \n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_dec\"]).attach(trainer, \"D_G_dec\")  \n",
    "ProgressBar().attach(trainer, metric_names=[\"Loss_G\",\"Loss_D\",\"D_real_enc\",\"D_real_dec\",\"D_fake_enc\",\"D_fake_dec\",\"D_G_L1\",\"D_G_dec\"])\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_losses(engine):\n",
    "    o = engine.state.output\n",
    "    print(o[\"D_G_L1\"])\n",
    "    for k in losses_key:\n",
    "        losses[k].append(o[k])\n",
    "\n",
    "best_l1 = 9999\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_checkpoint(engine):\n",
    "    global best_l1\n",
    "    if engine.state.output[\"D_G_L1\"] < best_l1:\n",
    "        best_l1 = engine.state.output[\"D_G_L1\"]\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': engine.state.epoch,\n",
    "        'model_state_dict': netG.state_dict(),\n",
    "        'optimizer_state_dict': optimizerG.state_dict(),\n",
    "        'loss': criterion,\n",
    "        'l1loss': loss,\n",
    "        'modelD_state_dict': netD.state_dict(),\n",
    "        'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "        }, 'model_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 00:25:39,672 ignite.distributed.launcher.Parallel INFO: Initialized processing group with backend: 'nccl'\n",
      "2023-06-12 00:25:39,673 ignite.distributed.launcher.Parallel INFO: - Run '<function training at 0x7f6927f12560>' in 1 processes\n",
      "                                                                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09745856374502182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06880583614110947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04088626056909561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06556326150894165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054947759956121445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06281029433012009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06265170127153397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061913829296827316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07148998975753784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06496509164571762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048162803053855896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]: [80/233]  34%|███▍      , Loss_G=0.0527, Loss_D=3.26, D_real_enc=1.17, D_real_dec=0.507, D_fake_enc=1.06, D_fake_dec=0.525, D_G_L1=0.0516, D_G_dec=1.17 [03:56<08:11]        "
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "def training(*args):\n",
    "    trainer.run(sony_dataloader, max_epochs=num_epoch)\n",
    "\n",
    "with idist.Parallel(backend='nccl') as parallel:\n",
    "    parallel.run(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epoch,\n",
    "            'model_state_dict': netG.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'modelD_state_dict': netD.state_dict(),\n",
    "            'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "            }, 'model_seed_{}.pt'.format(torch.random.initial_seed()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
